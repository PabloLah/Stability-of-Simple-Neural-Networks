{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d53283",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "0\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "1\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "#Note: We program a simple NN to interpolate a linear function.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#generating random data from a linear function and adding noise\n",
    "#generate x-values uniformly inside the interval [0,1]\n",
    "import numpy as np\n",
    "X_data = np.random.uniform(0, 1, 1000)\n",
    "\n",
    "#generate noise with normal distribution\n",
    "noise = np.random.normal(0, 0.1, X_data.size)\n",
    "\n",
    "#generate values of the linear function: f(x) = alpha_prime * x\n",
    "#and add the noise\n",
    "alpha_prime = 2\n",
    "y_data = alpha_prime * X_data + noise\n",
    "\n",
    "\n",
    "#building the model\n",
    "#import the necessary functions from the tensorflow library\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow import keras\n",
    "\n",
    "#introduce the learning model: one hidden layer and one hidden node\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_dim=1, activation='relu', use_bias=True))\n",
    "model.add(Dense(1, activation='linear', use_bias=True))\n",
    "\n",
    "\n",
    "#generate some points which lie on the graph alpha_prime/x (base points)\n",
    "#the x_values correspond to the beta-values of such points\n",
    "x_values = np.concatenate((np.linspace(0.7,np.sqrt(2),12),np.linspace(np.sqrt(2),3,8)))\n",
    "\n",
    "#compute the corresponding alpha_values\n",
    "y_values = alpha_prime/x_values\n",
    "\n",
    "#distance (along the gradient line) from points on the graph\n",
    "d=1\n",
    "\n",
    "#generating the new points; wich will give us the initial values of the weights\n",
    "#for a given point (x_base,2/x_base) we calculate the value (at the x-value x) of the perpendicular line on the \n",
    "#graph alpha_prime/x which goes through the point (x_base,2/x_base)\n",
    "\n",
    "#function of the gradient line at the position x_base\n",
    "def perpendicular(x_base,x):\n",
    "    return x_base**2 /alpha_prime * x + alpha_prime/x_base - x_base**3 /alpha_prime\n",
    "\n",
    "#calculate the x-value of the point which is a distance of d away from a base point on alpha_prime/x and which lies\n",
    "#on the perpendicular line to this base point\n",
    "\n",
    "#computes the x_value of the point which is distance d aways from x_base.. \n",
    "#..on the graph along the gradient line\n",
    "def x_distance(d, x_base):\n",
    "    return x_base + np.sqrt(d**2 * 1/(1+(x_base**4 / alpha_prime**2)))\n",
    "\n",
    "\n",
    "#compute the initial weights (beta,alpha)\n",
    "x_weight = x_distance(d, x_values) #betas\n",
    "y_weight = perpendicular(x_values, x_weight) #corresponding alphas\n",
    "\n",
    "\n",
    "#make plot larger so see small chnges better\n",
    "plt.figure(figsize=(6,6))\n",
    "#plt.axis('equal')\n",
    "plt.ylim((0,4))\n",
    "plt.xlim((0,4))\n",
    "plt.xlabel('beta-values')\n",
    "plt.ylabel('alpha-values')\n",
    "\n",
    "\n",
    "#for all initial weights (alpha,beta) optimize the model and plot where the points are sent\n",
    "l = len(x_values)\n",
    "\n",
    "#defining the bool vector which will allow us to store the tendency point pattern.\n",
    "#we will set bool_opt[i]=0, if the i-th point optimizes left of its initial gradient line\n",
    "#and bool_opt[i]=1, if the i-th point converges right of its initial gradient line and\n",
    "#bool_opt[i]=2, if the i-th point converges very close to its initial gradient line\n",
    "bool_opt = np.zeros(l)\n",
    "\n",
    "\n",
    "#learning rates for which we want to find the tendency point\n",
    "#note that we want to plot more smaller learning rates\n",
    "learning_rates = np.concatenate((np.linspace(0.005,0.07,16),np.linspace(0.07,1.15,8)))\n",
    "\n",
    "#here we will store the x-values of the computed tendency points\n",
    "x_tendency = np.zeros(len(learning_rates))\n",
    "\n",
    "#iterate through all learning rates\n",
    "for t in range(len(learning_rates)):\n",
    "    \n",
    "    #reset the boolean vector to store the tendency point pattern\n",
    "    #we will set bool_opt[i]=0, if the i-th point optimizes left of its initial gradient line\n",
    "    #and bool_opt[i]=1, if the i-th point converges right of its initial gradient line and\n",
    "    #bool_opt[i]=2, if the i-th point converges very close to its initial gradient line\n",
    "    bool_opt = np.zeros(l)\n",
    "    \n",
    "    #iterate through the x-values\n",
    "    for i in range(l):\n",
    "        beta_init = x_weight[i] #beta corresponds to x\n",
    "        alpha_init = y_weight[i]  #alpha corresponds to y\n",
    "        b_init = 0\n",
    "        h_init = 0\n",
    "\n",
    "        model.set_weights([np.array([[beta_init]]), np.array([b_init]), np.array([[alpha_init]]), np.array([h_init])])\n",
    "\n",
    "        #compile the keras model\n",
    "        #set some custom learning rate:\n",
    "        l_rate= learning_rates[t]\n",
    "\n",
    "        #set optimization parameters\n",
    "        opt = keras.optimizers.SGD(learning_rate=l_rate)\n",
    "        model.compile(loss='mean_squared_error', optimizer=opt)\n",
    "\n",
    "        #fit the keras model on the dataset\n",
    "        #verbose = 0, stops the algorithm from printing the progress after every epoch\n",
    "        model.fit(X_data, y_data, epochs=150, batch_size=20, verbose = 0)\n",
    "\n",
    "        #get the weights of the NN\n",
    "        [[[beta]], [b], [[alpha]], [h]] = model.get_weights()\n",
    "        \n",
    "        #stop the plotting if the optimized point does not lie on the graph alpha_real/x\n",
    "        if abs(alpha*beta - 2) > 0.3:\n",
    "            print('At the learning rate ',learning_rates[t], ' it did not optimize onto the graph.')\n",
    "            bool_opt = np.zeros(l)\n",
    "            break\n",
    "        \n",
    "\n",
    "        #plot point before and after optimization (and connect these points)\n",
    "        #plt.plot([beta_init, beta], [alpha_init,alpha], 'ro-')\n",
    "\n",
    "        #plot the gradient line on which the initial values lie\n",
    "        #x = np.linspace(0.2,4,30)\n",
    "        #y_perpline = perpendicular(x_values[i],x)\n",
    "        #plt.plot(x, y_perpline)\n",
    "\n",
    "        #check the initial weights optimized to the right of,  to the left of or exactly onto their gradient line\n",
    "        if abs(beta-x_values[i]) <= 0.001:\n",
    "                bool_opt[i] = 2\n",
    "        else: \n",
    "            if beta > x_values[i] : #so if the optimized point lies left of the gradient line\n",
    "                bool_opt[i] = 1\n",
    "            else:\n",
    "                bool_opt[i] = 0\n",
    "\n",
    "    #find out the approximate value of the tendency point\n",
    "    #x_tendency stays zero, if the vector does not have a desired form\n",
    "    \n",
    "    \n",
    "    #only during testing:\n",
    "    print(bool_opt)\n",
    "    print(t)\n",
    "    \n",
    "    for k in range(l-1):\n",
    "        if bool_opt[k] == 2:\n",
    "            x_tendency[t] = x_values[k]\n",
    "            break\n",
    "        if (bool_opt[k]) == 1.0 and (bool_opt[k+1]) == 0.0:\n",
    "            x_tendency[t] = (x_values[k] + x_values[k+1])/2\n",
    "            break\n",
    "    if bool_opt[l-1] == 2:\n",
    "        x_tendency[t] == x_values[l-1]\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "#plot the graph alpha_prime/x\n",
    "x = np.linspace(0.2,4,30)\n",
    "y = alpha_prime/x\n",
    "plt.plot(x,y)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#plot the learning rate vlaus on a log scale (so see the small values more precisely)\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.xlabel('learning rate')\n",
    "plt.ylabel('x_tendency')\n",
    "\n",
    "plt.plot(learning_rates, x_tendency, 'bo-')\n",
    "\n",
    "print('x-values: ', x_values)\n",
    "print('learning rates: ', learning_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364136ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
